#!/usr/bin/env python3
"""
Performance Profiling Script for PR-0003 Prime Log-Gap Experiment

This script profiles the performance of each major component of the pipeline
at different scales to understand the sub-linear scaling behavior.
"""

import sys
import os
import json
import time
import functools
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

# Add parent directories to path
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
src_dir = parent_dir / 'src'

sys.path.insert(0, str(src_dir))

import numpy as np

# Import experiment modules
from prime_generator import generate_primes_to_limit, compute_gaps
from binning import analyze_bins
from statistics import (
    linear_regression,
    kolmogorov_smirnov_tests,
    ljung_box_test,
    compute_acf_pacf,
    compute_skewness_kurtosis
)


class PerformanceProfiler:
    """
    Context manager and decorator for profiling code execution.
    """
    
    def __init__(self):
        self.timings = {}
        self.active_timers = {}
    
    def time(self, name: str):
        """Context manager for timing a code block."""
        return TimingContext(self, name)
    
    def start_timer(self, name: str):
        """Start a named timer."""
        self.active_timers[name] = time.time()
    
    def stop_timer(self, name: str):
        """Stop a named timer and record the duration."""
        if name in self.active_timers:
            duration = time.time() - self.active_timers[name]
            if name not in self.timings:
                self.timings[name] = []
            self.timings[name].append(duration)
            del self.active_timers[name]
            return duration
        return None
    
    def get_results(self) -> Dict[str, Any]:
        """Get profiling results with statistics."""
        results = {}
        for name, durations in self.timings.items():
            if len(durations) > 0:
                results[name] = {
                    'count': len(durations),
                    'total': sum(durations),
                    'mean': sum(durations) / len(durations),
                    'min': min(durations),
                    'max': max(durations),
                    'last': durations[-1]
                }
        return results
    
    def print_summary(self):
        """Print a human-readable summary of profiling results."""
        results = self.get_results()
        print("\n" + "=" * 70)
        print("PERFORMANCE PROFILING SUMMARY")
        print("=" * 70)
        
        if not results:
            print("No timing data collected.")
            return
        
        # Sort by total time
        sorted_results = sorted(results.items(), 
                               key=lambda x: x[1]['total'], 
                               reverse=True)
        
        total_time = sum(r['total'] for r in results.values())
        
        print(f"{'Component':<30} {'Time (s)':<12} {'% Total':<10} {'Calls':<8}")
        print("-" * 70)
        
        for name, stats in sorted_results:
            pct = (stats['total'] / total_time * 100) if total_time > 0 else 0
            print(f"{name:<30} {stats['total']:>10.3f}s {pct:>8.1f}% {stats['count']:>6d}")
        
        print("-" * 70)
        print(f"{'TOTAL':<30} {total_time:>10.3f}s {100.0:>8.1f}%")
        print("=" * 70)


class TimingContext:
    """Context manager for timing code blocks."""
    
    def __init__(self, profiler: PerformanceProfiler, name: str):
        self.profiler = profiler
        self.name = name
    
    def __enter__(self):
        self.profiler.start_timer(self.name)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.profiler.stop_timer(self.name)


def profile_at_scale(max_prime: int, n_bins: int = 100, use_cache: bool = False):
    """
    Profile the experiment at a specific scale.
    
    Args:
        max_prime: Maximum prime to generate
        n_bins: Number of bins for analysis
        use_cache: Whether to use disk cache
        
    Returns:
        Dictionary with profiling results and metadata
    """
    profiler = PerformanceProfiler()
    
    base_dir = Path(__file__).parent.parent
    data_dir = base_dir / 'data'
    
    # Create directories
    data_dir.mkdir(exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"PROFILING AT SCALE: {max_prime:,}")
    print(f"{'='*70}")
    
    total_start = time.time()
    
    # Phase 1: Prime Generation
    print("\n[1/6] Prime Generation...")
    with profiler.time("1_prime_generation"):
        cache_dir = str(data_dir) if use_cache else None
        primes = generate_primes_to_limit(max_prime, cache_dir=cache_dir, validate=True)
    
    prime_count = len(primes)
    print(f"  Generated {prime_count:,} primes")
    
    # Phase 2: Gap Computation
    print("\n[2/6] Gap Computation...")
    with profiler.time("2_gap_computation"):
        gaps_data = compute_gaps(primes, cache_dir=cache_dir, limit=max_prime)
    
    log_gaps = gaps_data['log_gaps']
    log_primes = gaps_data['log_primes']
    regular_gaps = gaps_data['regular_gaps']
    
    print(f"  Computed {len(log_gaps):,} gaps")
    
    # Phase 3: Binning Analysis
    print(f"\n[3/6] Binning Analysis ({n_bins} bins)...")
    with profiler.time("3_binning_analysis"):
        bin_analysis = analyze_bins(log_primes, log_gaps, n_bins=n_bins)
    
    print(f"  Used {bin_analysis['bins_used']}/{n_bins} bins")
    
    # Phase 4: Statistical Tests
    print("\n[4/6] Statistical Tests...")
    
    with profiler.time("4a_linear_regression"):
        regression = linear_regression(bin_analysis['mean'])
    
    with profiler.time("4b_ks_tests"):
        ks_tests = kolmogorov_smirnov_tests(log_gaps)
    
    with profiler.time("4c_acf_pacf"):
        acf_pacf = compute_acf_pacf(log_gaps, nlags=50)
    
    with profiler.time("4d_ljung_box"):
        ljung_box = ljung_box_test(log_gaps, max_lag=50)
    
    with profiler.time("4e_moments"):
        skew_kurt = compute_skewness_kurtosis(log_gaps)
    
    # Phase 5: Data Structures (memory profiling)
    print("\n[5/6] Memory Analysis...")
    memory_usage = {
        'primes_bytes': primes.nbytes,
        'log_gaps_bytes': log_gaps.nbytes,
        'log_primes_bytes': log_primes.nbytes,
        'regular_gaps_bytes': regular_gaps.nbytes,
        'total_bytes': primes.nbytes + log_gaps.nbytes + log_primes.nbytes + regular_gaps.nbytes
    }
    
    print(f"  Total memory: {memory_usage['total_bytes'] / 1024**2:.2f} MB")
    
    # Phase 6: Visualization (simulated - we'll measure setup time only)
    print("\n[6/6] Visualization Setup...")
    with profiler.time("6_visualization_prep"):
        # Simulate visualization preparation
        # In real run, this would include all plot generation
        time.sleep(0.1)  # Placeholder
    
    total_time = time.time() - total_start
    
    # Compile results
    timing_results = profiler.get_results()
    
    results = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'max_prime': max_prime,
            'n_bins': n_bins,
            'prime_count': prime_count,
            'gap_count': len(log_gaps),
            'total_time': total_time
        },
        'timings': timing_results,
        'memory': memory_usage,
        'statistical_summary': {
            'regression_slope': float(regression['slope']),
            'r_squared': float(regression['r_squared']),
            'best_fit': ks_tests['best_fit'],
            'skewness': float(skew_kurt['skewness']),
            'kurtosis': float(skew_kurt['kurtosis'])
        }
    }
    
    # Print summary
    profiler.print_summary()
    
    return results


def analyze_scaling(results_by_scale: dict):
    """
    Analyze scaling behavior across different scales.
    
    Args:
        results_by_scale: Dictionary mapping scale to profiling results
        
    Returns:
        Dictionary with scaling analysis
    """
    scales = sorted(results_by_scale.keys())
    
    if len(scales) < 2:
        print("Need at least 2 scales for scaling analysis")
        return {}
    
    print(f"\n{'='*70}")
    print("SCALING ANALYSIS")
    print(f"{'='*70}")
    
    scaling_analysis = {
        'scales': scales,
        'components': {}
    }
    
    # For each component, analyze how it scales
    component_names = set()
    for result in results_by_scale.values():
        component_names.update(result['timings'].keys())
    
    for component in sorted(component_names):
        times = []
        data_sizes = []
        
        for scale in scales:
            result = results_by_scale[scale]
            if component in result['timings']:
                times.append(result['timings'][component]['total'])
                data_sizes.append(result['metadata']['prime_count'])
        
        if len(times) >= 2:
            # Calculate scaling factor
            # If time2/time1 = (size2/size1)^α, then α = log(time2/time1) / log(size2/size1)
            time_ratio = times[-1] / times[0]
            size_ratio = data_sizes[-1] / data_sizes[0]
            
            if size_ratio > 1:
                scaling_exponent = np.log(time_ratio) / np.log(size_ratio)
            else:
                scaling_exponent = None
            
            scaling_analysis['components'][component] = {
                'times': times,
                'data_sizes': data_sizes,
                'time_ratio': float(time_ratio),
                'size_ratio': float(size_ratio),
                'scaling_exponent': float(scaling_exponent) if scaling_exponent is not None else None,
                'is_sublinear': bool(scaling_exponent < 1.0) if scaling_exponent is not None else None,
                'is_linear': bool(0.9 <= scaling_exponent <= 1.1) if scaling_exponent is not None else None
            }
    
    # Print scaling summary
    print(f"\n{'Component':<30} {'Exponent':<12} {'Scaling Type':<15}")
    print("-" * 70)
    
    for component, analysis in sorted(scaling_analysis['components'].items()):
        exp = analysis['scaling_exponent']
        if exp is not None:
            if exp < 0.9:
                scaling_type = "SUB-LINEAR"
            elif exp <= 1.1:
                scaling_type = "LINEAR"
            else:
                scaling_type = "SUPER-LINEAR"
            print(f"{component:<30} {exp:>10.4f} {scaling_type:<15}")
    
    print("=" * 70)
    
    # Overall efficiency factor
    overall_times = [results_by_scale[s]['metadata']['total_time'] for s in scales]
    overall_sizes = [results_by_scale[s]['metadata']['prime_count'] for s in scales]
    
    if len(overall_times) >= 2:
        overall_exponent = np.log(overall_times[-1] / overall_times[0]) / np.log(overall_sizes[-1] / overall_sizes[0])
        scaling_analysis['overall_scaling_exponent'] = float(overall_exponent)
        
        print(f"\nOVERALL SCALING EXPONENT: {overall_exponent:.4f}")
        if overall_exponent < 1.0:
            print(f"  → SUB-LINEAR scaling confirmed (efficiency factor ~{overall_exponent:.2f})")
        elif overall_exponent <= 1.1:
            print(f"  → LINEAR scaling")
        else:
            print(f"  → SUPER-LINEAR scaling")
    
    return scaling_analysis


def main():
    """Main profiling routine."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Profile PR-0003 Prime Log-Gap Experiment Performance'
    )
    parser.add_argument('--scales', type=str, default='1e6,1e7',
                       help='Comma-separated list of scales to profile (default: 1e6,1e7)')
    parser.add_argument('--bins', type=int, default=100,
                       help='Number of bins (default: 100)')
    parser.add_argument('--no-cache', action='store_true',
                       help='Disable caching (force regeneration)')
    parser.add_argument('--output', type=str, default='profiling_results.json',
                       help='Output file for results (default: profiling_results.json)')
    
    args = parser.parse_args()
    
    # Parse scales
    scales = [int(float(s)) for s in args.scales.split(',')]
    
    print("=" * 70)
    print("PR-0003 PERFORMANCE PROFILING")
    print("=" * 70)
    print(f"Scales to profile: {', '.join(str(s) for s in scales)}")
    print(f"Bins: {args.bins}")
    print(f"Use cache: {not args.no_cache}")
    print()
    
    # Profile each scale
    results_by_scale = {}
    for scale in scales:
        try:
            result = profile_at_scale(
                max_prime=scale,
                n_bins=args.bins,
                use_cache=not args.no_cache
            )
            results_by_scale[scale] = result
        except Exception as e:
            print(f"\nERROR profiling scale {scale}: {e}")
            import traceback
            traceback.print_exc()
    
    # Analyze scaling
    if len(results_by_scale) >= 2:
        scaling_analysis = analyze_scaling(results_by_scale)
    else:
        scaling_analysis = {}
    
    # Save results
    output_file = Path(__file__).parent.parent / args.output
    full_results = {
        'profiling_date': datetime.now().isoformat(),
        'scales_profiled': scales,
        'results_by_scale': {str(k): v for k, v in results_by_scale.items()},
        'scaling_analysis': scaling_analysis
    }
    
    with open(output_file, 'w') as f:
        json.dump(full_results, f, indent=2)
    
    print(f"\n\nResults saved to: {output_file}")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
