
# TECH-SPEC: Falsifying Claims via Hybrid Model Comparison

## 1. Objective

Test whether the **claimed properties of prime gaps** (lognormal distribution, high autocorrelation, φ-harmonic structure) can be **replicated by hybrid models** that combine simple stochastic processes with tunable parameters, thereby demonstrating these properties are not unique to primes.

The experiment should answer:

- Can **hybrid models** (e.g., ARMA + lognormal noise, Cramér model with drift) replicate the statistical signatures claimed for prime gaps?
- Are the observed patterns **distinguishable** from those generated by parameter-tuned synthetic models?
- Do hybrid models require **fine-tuning** to match observations, or do they naturally produce similar statistics?
- Can hybrid models pass the **same statistical tests** that prime gaps allegedly pass?

---

## 2. High-level design

### Core methodology:

- Define a suite of **hybrid stochastic models** that combine:
  - **Base distributions**: Lognormal, exponential, Poisson
  - **Temporal dependencies**: ARMA(p,q), GARCH(1,1), fractional Brownian motion
  - **Additive structure**: Trend + periodic + noise components
- For each hybrid model:
  - Tune parameters to **approximately match** key prime gap statistics (mean, variance, ACF(1), tail behavior)
  - Generate synthetic gap sequences of length N matching prime gap datasets
- Compare synthetic sequences against real prime gaps using:
  - **Distribution tests**: KS test, Anderson-Darling, QQ-plots
  - **Autocorrelation tests**: ACF comparison, Ljung-Box test
  - **Spectral tests**: Power spectrum, φ-harmonic alignment
- Apply **model selection criteria**: AIC, BIC to evaluate goodness-of-fit

### Null hypothesis:

If prime gap properties are **not unique**:
- Hybrid models with moderate parameter tuning can replicate observed statistics.
- Synthetic sequences pass the same tests as prime gaps.
- No single hybrid model is clearly "worse" than others at matching data.

### Falsification criteria:

- **Claim falsified** if hybrid models can replicate prime gap statistics without excessive tuning.
- **Claim supported** if NO hybrid model can match prime gap patterns despite parameter optimization.

---

## 3. Data and inputs

### Prime gap datasets:

- **Prime log-gaps** Δ_n = ln(p_{n+1} / p_n) from ranges:
  1. [10^6, 10^7] (N ≈ 500k)
  2. [10^9, 10^10] (N ≈ 400k)
  3. [10^12, 10^13] (N ≈ 300k)

### Target statistics to match:

From real prime gaps:
- **Mean μ**, **Standard deviation σ**
- **ACF(1), ACF(2), ..., ACF(10)**
- **Skewness, Kurtosis**
- **95th and 99th percentile tail values**
- **Power spectrum peaks** (if any)

---

## 4. Hybrid models

### Model 1: Lognormal ARMA(1,1)

**Structure**:
```
Δ_n = exp(X_n)
X_n = φ X_{n-1} + ε_n + θ ε_{n-1}
ε_n ~ N(0, σ²)
```

**Free parameters**: φ (AR coefficient), θ (MA coefficient), σ (noise std)

**Tuning strategy**: Grid search over (φ, θ, σ) to minimize distance to target ACF(1:10)

### Model 2: Exponential + Drift

**Structure**:
```
Δ_n = λ + β n + exp(ε_n)
ε_n ~ N(0, σ²)
```

**Free parameters**: λ (baseline), β (drift), σ (noise std)

**Tuning strategy**: Least squares fit to match mean and variance trends

### Model 3: Fractional Gaussian noise + Lognormal base

**Structure**:
```
Δ_n = exp(μ + fGn(H))
```
where fGn(H) is fractional Gaussian noise with Hurst exponent H.

**Free parameters**: μ (location), H ∈ [0, 1] (long-range dependence)

**Tuning strategy**: Estimate H from observed ACF decay rate

### Model 4: GARCH(1,1) + Lognormal

**Structure**:
```
X_n = μ + σ_n ε_n
σ_n² = ω + α (X_{n-1} - μ)² + β σ_{n-1}²
Δ_n = exp(X_n)
```

**Free parameters**: μ, ω, α, β

**Tuning strategy**: Maximum likelihood estimation

### Model 5: Cramér model with correlated gaps

**Structure**:
```
Δ_n ~ Poisson(λ) with gaps correlated via:
Cov(Δ_n, Δ_{n+k}) = ρ^k
```

**Free parameters**: λ (rate), ρ (correlation decay)

**Tuning strategy**: Match mean gap and ACF(1)

### Model 6: Additive decomposition

**Structure**:
```
Δ_n = Trend(n) + Seasonal(n) + Noise(n)
Trend(n) = a + b ln(n)
Seasonal(n) = A sin(2π n / P)
Noise(n) ~ Lognormal(μ, σ)
```

**Free parameters**: a, b, A, P, μ, σ

**Tuning strategy**: Fourier analysis for seasonal component, regression for trend

---

## 5. Metrics and analysis

### Primary metrics:

1. **Statistical distance**:
   ```
   D_KS = max|F_real(x) - F_synthetic(x)|
   ```
   (Kolmogorov-Smirnov distance)

2. **ACF error**:
   ```
   E_ACF = √[Σ_k (ACF_real(k) - ACF_synthetic(k))²]
   ```

3. **Tail discrepancy**:
   ```
   Δ_tail = |P95_real - P95_synthetic| + |P99_real - P99_synthetic|
   ```

4. **Model fit (AIC)**:
   ```
   AIC = 2k - 2 ln(L)
   ```
   where k = number of parameters, L = likelihood

### Statistical tests:

1. **Two-sample KS test**: H0: Real and synthetic come from same distribution
2. **Ljung-Box test**: Compare ACF structure
3. **Spectral comparison**: Chi-squared test on power spectrum bins
4. **QQ-plot analysis**: Visual + R² goodness-of-fit

### Visualizations:

- **Distribution overlay**: Real vs synthetic histograms
- **ACF comparison**: Real vs synthetic ACF(k) for k=1...20
- **QQ-plots**: Quantile-quantile for each hybrid model
- **Parameter sensitivity**: How small changes in parameters affect match quality

---

## 6. Falsification criteria

### Evidence FALSIFYING uniqueness claim:

✓ ≥ 2 hybrid models achieve D_KS < 0.05 (indistinguishable distributions)  
✓ ACF error E_ACF < 0.1 for at least one model  
✓ Synthetic sequences pass KS test with p > 0.05  
✓ Models require only "reasonable" parameter tuning (not extreme fine-tuning)  
✓ AIC/BIC comparable between hybrid models and implicit "prime model"

### Evidence SUPPORTING uniqueness claim:

✗ NO hybrid model achieves D_KS < 0.1 even with extensive tuning  
✗ All models fail to match ACF(1) by more than 0.3  
✗ Synthetic sequences systematically fail distribution tests (p < 0.01)  
✗ Prime gaps have unique statistical signature not captured by any combination  
✗ "Best" hybrid model requires unrealistic parameter values

**Decision rule**:
- If ≥ 2 models pass KS test (p > 0.05) AND E_ACF < 0.15 → **Claims falsified** (not unique)
- If ALL models fail with D_KS > 0.2 AND E_ACF > 0.5 → **Uniqueness supported**

---

## 7. Expected outputs

### Quantitative results:

1. **Table**: Model comparison (D_KS, E_ACF, AIC, p-value) for each hybrid
2. **Best-fit parameters**: Optimized values for each model
3. **Test outcomes**: KS test, Ljung-Box results for each model
4. **Ranking**: Models sorted by goodness-of-fit

### Plots:

1. `distribution_comparison.png`: Histogram overlays (real vs each model)
2. `acf_comparison.png`: ACF plots for real + all hybrid models
3. `qq_plots.png`: Grid of QQ-plots for each model
4. `parameter_sensitivity.png`: Heatmaps showing fit quality vs parameter values
5. `model_ranking.png`: Bar chart of D_KS and E_ACF across models

### Report:

- `hybrid_model_report.md`: Model specifications, tuning process, test results, conclusions

---

## 8. Implementation notes

### Python libraries:

```python
import numpy as np
from scipy import stats
from statsmodels.tsa.arima.model import ARIMA
from arch import arch_model  # For GARCH
import matplotlib.pyplot as plt
```

### Algorithm outline:

```python
models = [ARMA_lognormal, exponential_drift, fGn_lognormal, GARCH_lognormal, correlated_cramer, additive_decomposition]

prime_gaps = load_prime_gaps(range)
target_stats = compute_stats(prime_gaps)

results = {}
for model in models:
    # Parameter tuning
    best_params = optimize_parameters(model, target_stats)
    
    # Generate synthetic data
    synthetic_gaps = model.simulate(best_params, N=len(prime_gaps))
    
    # Compute metrics
    d_ks, p_val = stats.ks_2samp(prime_gaps, synthetic_gaps)
    acf_error = compute_acf_error(prime_gaps, synthetic_gaps)
    aic = compute_aic(model, synthetic_gaps)
    
    # Store results
    results[model.name] = {
        'params': best_params,
        'd_ks': d_ks,
        'p_value': p_val,
        'acf_error': acf_error,
        'aic': aic
    }

# Rank models
ranked = sorted(results.items(), key=lambda x: x[1]['d_ks'])
```

### Validation checks:

- Verify synthetic data generation is correct (check moments)
- Ensure optimization converges (multiple starting points)
- Check parameter bounds are reasonable

---

## 9. Timeline

- **Day 1-2**: Implement all 6 hybrid models, verify simulation code
- **Day 3-4**: Parameter tuning and optimization for each model
- **Day 5**: Compute metrics, run statistical tests
- **Day 6**: Create comparison plots, analyze sensitivity
- **Day 7**: Write report, draw conclusions about claim falsification

---

## 10. Potential pitfalls

1. **Overfitting**: Too many free parameters → limit to ≤ 5 parameters per model
2. **Local optima**: Optimization may get stuck → use multiple random restarts
3. **Sample size**: Small N reduces test power → ensure N ≥ 10,000
4. **Model complexity**: More complex ≠ better → penalize via AIC/BIC
5. **Cherry-picking**: Test ALL models, report ALL results (no selective reporting)

---

## 11. Interpretation guidelines

If hybrid models CAN match prime gap statistics:
→ Claimed properties are **not unique** to primes
→ Statistical patterns may be **generic** features of autocorrelated lognormal processes
→ Claims about "special structure" are **falsified**

If NO hybrid model can match:
→ Prime gaps may have **genuine unique structure**
→ Further investigation needed into **why** primes are special
→ Claims gain **partial support** (though not proof)

---

## 12. References

- **Box & Jenkins (1976)**: *Time Series Analysis*. (ARMA models)
- **Bollerslev (1986)**: *Generalized autoregressive conditional heteroskedasticity*. (GARCH)
- **Mandelbrot & Van Ness (1968)**: *Fractional Brownian motions*. (fGn, long-range dependence)
- **Burnham & Anderson (2002)**: *Model Selection and Multimodel Inference*. (AIC/BIC)
