#!/usr/bin/env python3
"""
Hypothesis Falsification Test: AI-Augmented Scientific Falsification Pipeline

This experiment tests the following hypothesis:
The repository pioneers an AI-augmented scientific falsification pipeline for prime gap
modeling, revealing inconsistencies in traditional exponential distributions via iterative
experiments that integrate high-precision prime generation, potentially enabling more robust
cryptographic prime selection by identifying fractal-like cascades and hybrid patterns in
gap sequences.

It also tests the reversed hierarchy discovery mechanism where bottom-up pattern detection
outperforms top-down assumptions, leading to fewer false positives in distribution falsification.
"""

import sys
import os
from pathlib import Path
from typing import Dict, List, Tuple, Any
import json
from datetime import datetime

# Add parent directories to path for imports
sys.path.append(str(Path(__file__).parent.parent))


class ExperimentConfig:
    """Configuration for the falsification test experiment"""
    
    def __init__(self):
        """IMPLEMENTED: Initialize experiment configuration with default values"""
        self.experiment_name = "hypothesis_falsification_test"
        self.base_dir = Path(__file__).parent
        self.results_dir = self.base_dir / "results"
        self.data_dir = self.base_dir / "data"
        self.src_dir = self.base_dir / "src"
        
        # Experiment parameters
        self.seed = 42
        self.num_trials = 10
        self.prime_ranges = [
            (1_000_000, 10_000_000),
            (10_000_000, 100_000_000),
        ]
        
        # Falsification thresholds
        self.false_positive_threshold = 0.05
        self.confidence_level = 0.95
        
        # Ensure directories exist
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.src_dir.mkdir(parents=True, exist_ok=True)
    
    def to_dict(self) -> Dict[str, Any]:
        """IMPLEMENTED: Convert configuration to dictionary for serialization"""
        return {
            "experiment": {
                "name": self.experiment_name,
                "base_dir": str(self.base_dir),
            },
            "directories": {
                "results": str(self.results_dir),
                "data": str(self.data_dir),
                "src": str(self.src_dir),
            },
            "parameters": {
                "seed": self.seed,
                "num_trials": self.num_trials,
                "prime_ranges": self.prime_ranges,
            },
            "thresholds": {
                "false_positive_threshold": self.false_positive_threshold,
                "confidence_level": self.confidence_level,
            }
        }
    
    def save(self, filepath: Path):
        """IMPLEMENTED: Save configuration to JSON file"""
        config_dict = self.to_dict()  # [IMPLEMENTED ✓]
        with open(filepath, 'w') as f:
            json.dump(config_dict, f, indent=2)


class FalsificationPipeline:
    """Core falsification pipeline implementation"""
    
    def __init__(self, config: ExperimentConfig):
        """IMPLEMENTED: Initialize the falsification pipeline"""
        import random
        import numpy as np
        
        self.config = config
        self.results = {}
        
        # Set random seeds for reproducibility
        random.seed(config.seed)
        np.random.seed(config.seed)
        
        # Initialize experiment metadata
        self.experiment_id = f"experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.start_time = datetime.now()
        
        print(f"[{self.start_time}] Initialized FalsificationPipeline")
        print(f"  Experiment ID: {self.experiment_id}")
        print(f"  Random seed: {config.seed}")
    
    def generate_prime_gaps(self, start: int, end: int) -> List[int]:
        """IMPLEMENTED: Generate prime gaps for testing in given range"""
        import numpy as np
        
        print(f"  Generating primes in range [{start}, {end}]...")
        
        # Use simple sieve of Eratosthenes for moderate ranges
        if end - start > 100_000_000:
            raise ValueError(f"Range too large: {end-start}. Consider smaller ranges.")
        
        # Generate primes using sieve
        def sieve_of_eratosthenes(limit):
            """Simple sieve for generating primes up to limit"""
            if limit < 2:
                return np.array([], dtype=np.int64)
            
            is_prime = np.ones(limit + 1, dtype=bool)
            is_prime[0:2] = False
            
            for i in range(2, int(np.sqrt(limit)) + 1):
                if is_prime[i]:
                    is_prime[i*i:limit+1:i] = False
            
            return np.where(is_prime)[0]
        
        # Generate all primes up to end
        all_primes = sieve_of_eratosthenes(end)
        
        # Filter to range [start, end]
        primes_in_range = all_primes[all_primes >= start]
        
        if len(primes_in_range) < 2:
            print(f"  WARNING: Only {len(primes_in_range)} prime(s) found in range")
            return []
        
        # Calculate gaps
        gaps = np.diff(primes_in_range).tolist()
        
        print(f"  Generated {len(primes_in_range)} primes, {len(gaps)} gaps")
        return gaps
    
    def test_exponential_distribution(self, gaps: List[int]) -> Dict[str, float]:
        """IMPLEMENTED: Test if gaps follow exponential distribution"""
        import numpy as np
        from scipy import stats
        
        gaps_array = np.array(gaps, dtype=float)
        
        # Fit exponential distribution (single parameter: scale = 1/lambda)
        # scipy.stats.expon uses scale parameter
        scale_param = np.mean(gaps_array)  # MLE for exponential
        
        # Perform Kolmogorov-Smirnov test
        ks_statistic, ks_pvalue = stats.kstest(
            gaps_array, 
            lambda x: stats.expon.cdf(x, scale=scale_param)
        )
        
        # Perform Anderson-Darling test for exponential
        ad_result = stats.anderson(gaps_array, dist='expon')
        
        # Calculate log-likelihood
        log_likelihood = np.sum(stats.expon.logpdf(gaps_array, scale=scale_param))
        
        # Calculate AIC and BIC
        n = len(gaps_array)
        k = 1  # number of parameters (scale)
        aic = 2 * k - 2 * log_likelihood
        bic = k * np.log(n) - 2 * log_likelihood
        
        return {
            'distribution': 'exponential',
            'scale': scale_param,
            'ks_statistic': ks_statistic,
            'ks_pvalue': ks_pvalue,
            'ad_statistic': ad_result.statistic,
            'ad_critical_values': ad_result.critical_values.tolist(),
            'log_likelihood': log_likelihood,
            'aic': aic,
            'bic': bic,
            'n_samples': n,
        }
    
    def test_lognormal_distribution(self, gaps: List[int]) -> Dict[str, float]:
        """IMPLEMENTED: Test if gaps follow lognormal distribution"""
        import numpy as np
        from scipy import stats
        
        gaps_array = np.array(gaps, dtype=float)
        
        # Remove zeros if any (lognormal requires positive values)
        gaps_array = gaps_array[gaps_array > 0]
        
        # Fit lognormal distribution
        # Returns shape (sigma), loc (usually 0), scale (exp(mu))
        shape, loc, scale = stats.lognorm.fit(gaps_array, floc=0)
        
        # Perform Kolmogorov-Smirnov test
        ks_statistic, ks_pvalue = stats.kstest(
            gaps_array,
            lambda x: stats.lognorm.cdf(x, shape, loc, scale)
        )
        
        # Anderson-Darling (using normal distribution on log-transformed data)
        log_gaps = np.log(gaps_array)
        ad_result = stats.anderson(log_gaps, dist='norm')
        
        # Calculate log-likelihood
        log_likelihood = np.sum(stats.lognorm.logpdf(gaps_array, shape, loc, scale))
        
        # Calculate AIC and BIC
        n = len(gaps_array)
        k = 2  # number of parameters (shape, scale; loc fixed at 0)
        aic = 2 * k - 2 * log_likelihood
        bic = k * np.log(n) - 2 * log_likelihood
        
        return {
            'distribution': 'lognormal',
            'shape': shape,  # sigma in log-space
            'scale': scale,  # exp(mu)
            'loc': loc,
            'ks_statistic': ks_statistic,
            'ks_pvalue': ks_pvalue,
            'ad_statistic': ad_result.statistic,
            'ad_critical_values': ad_result.critical_values.tolist(),
            'log_likelihood': log_likelihood,
            'aic': aic,
            'bic': bic,
            'n_samples': n,
        }
    
    def compare_distributions(self, exp_results: Dict, lognorm_results: Dict) -> Dict[str, Any]:
        """IMPLEMENTED: Compare exponential vs lognormal fit quality"""
        # Extract AIC/BIC values
        aic_exp = exp_results['aic']
        aic_lognorm = lognorm_results['aic']
        bic_exp = exp_results['bic']
        bic_lognorm = lognorm_results['bic']
        
        # Calculate deltas (positive favors lognormal)
        delta_aic = aic_exp - aic_lognorm
        delta_bic = bic_exp - bic_lognorm
        
        # Determine winner based on BIC (more conservative)
        if delta_bic >= 10:
            winner = 'lognormal'
            confidence = 'strong'
        elif delta_bic >= 6:
            winner = 'lognormal'
            confidence = 'moderate'
        elif delta_bic >= 2:
            winner = 'lognormal'
            confidence = 'weak'
        elif delta_bic <= -10:
            winner = 'exponential'
            confidence = 'strong'
        elif delta_bic <= -6:
            winner = 'exponential'
            confidence = 'moderate'
        elif delta_bic <= -2:
            winner = 'exponential'
            confidence = 'weak'
        else:
            winner = 'inconclusive'
            confidence = 'none'
        
        # Compare KS test p-values
        better_ks = 'lognormal' if lognorm_results['ks_pvalue'] > exp_results['ks_pvalue'] else 'exponential'
        
        return {
            'delta_aic': delta_aic,
            'delta_bic': delta_bic,
            'winner': winner,
            'confidence': confidence,
            'better_ks': better_ks,
            'exp_ks_pvalue': exp_results['ks_pvalue'],
            'lognorm_ks_pvalue': lognorm_results['ks_pvalue'],
        }
    
    def detect_fractal_cascade(self, gaps: List[int]) -> Dict[str, Any]:
        """IMPLEMENTED: Test for fractal cascade structure in gaps"""
        import numpy as np
        from scipy import stats
        
        gaps_array = np.array(gaps, dtype=float)
        
        # Stratify gaps into quintiles (5 strata) by magnitude
        quintiles = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
        quantile_values = np.quantile(gaps_array, quintiles)
        
        strata_results = []
        
        for i in range(len(quintiles) - 1):
            lower = quantile_values[i]
            upper = quantile_values[i + 1]
            
            # Get gaps in this stratum
            if i == len(quintiles) - 2:  # Last stratum, include upper bound
                stratum_gaps = gaps_array[(gaps_array >= lower) & (gaps_array <= upper)]
            else:
                stratum_gaps = gaps_array[(gaps_array >= lower) & (gaps_array < upper)]
            
            if len(stratum_gaps) < 10:  # Skip if too few samples
                continue
            
            # Fit lognormal to stratum
            log_gaps = np.log(stratum_gaps[stratum_gaps > 0])
            mu = np.mean(log_gaps)
            sigma = np.std(log_gaps, ddof=1)
            
            strata_results.append({
                'stratum': i,
                'lower': lower,
                'upper': upper,
                'n_samples': len(stratum_gaps),
                'mu': mu,  # mean in log-space
                'sigma': sigma,  # std in log-space
                'exp_mu': np.exp(mu),  # geometric mean
            })
        
        # Extract mu and sigma for power law fitting
        if len(strata_results) < 3:
            return {
                'fractal_detected': False,
                'reason': 'insufficient_strata',
                'strata_results': strata_results,
            }
        
        exp_mus = np.array([s['exp_mu'] for s in strata_results])
        sigmas = np.array([s['sigma'] for s in strata_results])
        
        # Fit power law: σ ∼ μ^H (in log-log space: log(σ) ∼ H * log(μ))
        # Remove any invalid values
        valid_idx = (exp_mus > 0) & (sigmas > 0) & np.isfinite(exp_mus) & np.isfinite(sigmas)
        exp_mus_valid = exp_mus[valid_idx]
        sigmas_valid = sigmas[valid_idx]
        
        if len(exp_mus_valid) < 3:
            return {
                'fractal_detected': False,
                'reason': 'insufficient_valid_data',
                'strata_results': strata_results,
            }
        
        # Log-log regression
        log_exp_mus = np.log(exp_mus_valid)
        log_sigmas = np.log(sigmas_valid)
        
        slope, intercept, r_value, p_value, std_err = stats.linregress(log_exp_mus, log_sigmas)
        
        # Hurst exponent is the slope
        hurst_exponent = slope
        
        # Check if H is in expected range [0.7, 0.9] for fractal cascades
        in_expected_range = 0.7 <= hurst_exponent <= 0.9
        
        return {
            'fractal_detected': in_expected_range and r_value**2 > 0.5,
            'hurst_exponent': hurst_exponent,
            'hurst_std_err': std_err,
            'r_squared': r_value**2,
            'p_value': p_value,
            'intercept': intercept,
            'in_expected_range': in_expected_range,
            'expected_range': [0.7, 0.9],
            'n_strata': len(strata_results),
            'strata_results': strata_results,
        }
    
    def run_bottom_up_analysis(self, gaps: List[int]) -> Dict[str, Any]:
        """IMPLEMENTED: Perform bottom-up pattern detection on gaps"""
        import numpy as np
        from math import factorial, log
        
        gaps_array = np.array(gaps, dtype=float)
        n_gaps = len(gaps_array)
        
        # Estimate corresponding n (number of primes) for moment asymptotics
        # Using rough estimate that n ≈ number of gaps
        n_estimate = n_gaps
        log_n = log(n_estimate) if n_estimate > 1 else 1.0
        
        # Compute empirical moments (k=1,2,3,4)
        moments = {}
        moment_deviations = {}
        
        for k in range(1, 5):
            # Empirical k-th moment
            empirical_moment = np.mean(gaps_array ** k)
            moments[k] = empirical_moment
            
            # Expected k-th moment for exponential: k! × (log n)^k
            factorial_k = factorial(k)
            expected_moment = factorial_k * (log_n ** k)
            
            # Relative deviation: A_k = empirical / expected - 1
            if expected_moment > 0:
                deviation = (empirical_moment / expected_moment) - 1
            else:
                deviation = float('inf')
            
            moment_deviations[k] = deviation
        
        # Check for reversed hierarchy: |A_4| < |A_3| < |A_2| < |A_1|
        abs_deviations = {k: abs(moment_deviations[k]) for k in range(1, 5)}
        reversed_hierarchy = (
            abs_deviations[4] < abs_deviations[3] < 
            abs_deviations[2] < abs_deviations[1]
        )
        
        # Build distribution hypothesis from patterns
        # If higher moments converge faster → lognormal/fractal structure
        # If normal hierarchy → simpler exponential
        if reversed_hierarchy:
            hypothesis = 'lognormal_fractal'
            confidence = 'high' if abs_deviations[1] - abs_deviations[4] > 0.02 else 'moderate'
        else:
            hypothesis = 'exponential_or_mixed'
            confidence = 'low'
        
        return {
            'approach': 'bottom_up',
            'moments': moments,
            'moment_deviations': moment_deviations,
            'abs_deviations': abs_deviations,
            'reversed_hierarchy': reversed_hierarchy,
            'hypothesis': hypothesis,
            'confidence': confidence,
            'n_samples': n_gaps,
            'log_n_estimate': log_n,
        }
    
    def run_top_down_analysis(self, gaps: List[int]) -> Dict[str, Any]:
        """IMPLEMENTED: Perform top-down pattern detection with assumed distribution"""
        import numpy as np
        from scipy import stats
        
        gaps_array = np.array(gaps, dtype=float)
        
        # Start with Cramér model assumption: gaps ~ Exponential
        # Fit exponential distribution
        scale_param = np.mean(gaps_array)
        
        # Test goodness of fit using KS test
        ks_statistic, ks_pvalue = stats.kstest(
            gaps_array,
            lambda x: stats.expon.cdf(x, scale=scale_param)
        )
        
        # Determine if exponential assumption holds
        # Typically use p-value threshold of 0.05
        exponential_fits = ks_pvalue > 0.05
        
        hypothesis = 'exponential'
        iterations = 1
        
        # If exponential fails, try lognormal (iteration 2)
        if not exponential_fits:
            iterations = 2
            gaps_positive = gaps_array[gaps_array > 0]
            shape, loc, scale = stats.lognorm.fit(gaps_positive, floc=0)
            
            ks_ln_stat, ks_ln_pvalue = stats.kstest(
                gaps_positive,
                lambda x: stats.lognorm.cdf(x, shape, loc, scale)
            )
            
            if ks_ln_pvalue > ks_pvalue:
                hypothesis = 'lognormal'
                final_pvalue = ks_ln_pvalue
            else:
                hypothesis = 'exponential_forced'
                final_pvalue = ks_pvalue
        else:
            final_pvalue = ks_pvalue
        
        # Count false positives: cases where we accept exponential when it's not good
        # Conservative: if p-value < 0.1, it's a borderline case
        false_positive_risk = 1 if (exponential_fits and ks_pvalue < 0.1) else 0
        
        return {
            'approach': 'top_down',
            'initial_assumption': 'exponential',
            'exponential_fits': exponential_fits,
            'iterations_required': iterations,
            'final_hypothesis': hypothesis,
            'ks_pvalue': final_pvalue,
            'false_positive_risk': false_positive_risk,
            'n_samples': len(gaps_array),
        }
    
    def compare_approaches(self, bottom_up: Dict, top_down: Dict) -> Dict[str, Any]:
        """IMPLEMENTED: Compare bottom-up vs top-down analysis effectiveness"""
        # Extract metrics from both approaches
        bu_hypothesis = bottom_up['hypothesis']
        bu_confidence = bottom_up['confidence']
        bu_reversed_hierarchy = bottom_up['reversed_hierarchy']
        
        td_hypothesis = top_down['final_hypothesis']
        td_iterations = top_down['iterations_required']
        td_false_positive_risk = top_down['false_positive_risk']
        
        # Determine agreement
        # Map hypotheses to comparable categories
        bu_category = 'complex' if 'lognormal' in bu_hypothesis else 'simple'
        td_category = 'complex' if 'lognormal' in td_hypothesis else 'simple'
        
        approaches_agree = bu_category == td_category
        
        # Bottom-up advantages:
        # 1. Detects reversed hierarchy (more nuanced pattern)
        # 2. No iterations needed (direct from data)
        # 3. Provides confidence levels
        
        # Top-down disadvantages:
        # 1. May require multiple iterations
        # 2. Has false positive risk
        # 3. Starts with assumption that may be wrong
        
        # Score each approach
        bu_score = 0
        if bu_reversed_hierarchy:
            bu_score += 2  # Detected advanced pattern
        if bu_confidence in ['high', 'moderate']:
            bu_score += 1
        
        td_score = 0
        if td_iterations == 1:
            td_score += 1  # Got it right first try
        if td_false_positive_risk == 0:
            td_score += 1
        
        # Determine winner
        if bu_score > td_score:
            winner = 'bottom_up'
        elif td_score > bu_score:
            winner = 'top_down'
        else:
            winner = 'tie'
        
        return {
            'bottom_up_hypothesis': bu_hypothesis,
            'top_down_hypothesis': td_hypothesis,
            'approaches_agree': approaches_agree,
            'bottom_up_score': bu_score,
            'top_down_score': td_score,
            'winner': winner,
            'bottom_up_advantages': {
                'reversed_hierarchy_detected': bu_reversed_hierarchy,
                'confidence_provided': bu_confidence,
                'direct_from_data': True,
            },
            'top_down_disadvantages': {
                'iterations_required': td_iterations,
                'false_positive_risk': td_false_positive_risk,
                'assumption_based': True,
            }
        }
    
    def run_single_trial(self, trial_id: int, prime_range: Tuple[int, int]) -> Dict[str, Any]:
        """IMPLEMENTED: Execute a single experimental trial"""
        print(f"\n{'='*60}")
        print(f"TRIAL {trial_id}: Range [{prime_range[0]:,}, {prime_range[1]:,}]")
        print(f"{'='*60}")
        
        start, end = prime_range
        
        # Generate prime gaps [IMPLEMENTED ✓]
        gaps = self.generate_prime_gaps(start, end)
        
        if len(gaps) == 0:
            return {
                'trial_id': trial_id,
                'prime_range': prime_range,
                'status': 'failed',
                'reason': 'no_gaps_generated',
            }
        
        # Test exponential distribution [IMPLEMENTED ✓]
        print("  Testing exponential distribution...")
        exp_results = self.test_exponential_distribution(gaps)
        
        # Test lognormal distribution [IMPLEMENTED ✓]
        print("  Testing lognormal distribution...")
        lognorm_results = self.test_lognormal_distribution(gaps)
        
        # Compare distributions [IMPLEMENTED ✓]
        print("  Comparing distributions...")
        dist_comparison = self.compare_distributions(exp_results, lognorm_results)
        
        # Detect fractal cascades [IMPLEMENTED ✓]
        print("  Detecting fractal cascade structure...")
        fractal_results = self.detect_fractal_cascade(gaps)
        
        # Run bottom-up analysis [IMPLEMENTED ✓]
        print("  Running bottom-up analysis...")
        bottom_up_results = self.run_bottom_up_analysis(gaps)
        
        # Run top-down analysis [IMPLEMENTED ✓]
        print("  Running top-down analysis...")
        top_down_results = self.run_top_down_analysis(gaps)
        
        # Compare approaches [IMPLEMENTED ✓]
        print("  Comparing approaches...")
        approach_comparison = self.compare_approaches(bottom_up_results, top_down_results)
        
        # Aggregate results
        trial_result = {
            'trial_id': trial_id,
            'prime_range': prime_range,
            'n_gaps': len(gaps),
            'status': 'success',
            'exponential_test': exp_results,
            'lognormal_test': lognorm_results,
            'distribution_comparison': dist_comparison,
            'fractal_cascade': fractal_results,
            'bottom_up_analysis': bottom_up_results,
            'top_down_analysis': top_down_results,
            'approach_comparison': approach_comparison,
        }
        
        print(f"  Result: {dist_comparison['winner']} (ΔBIC={dist_comparison['delta_bic']:.2f})")
        print(f"  Fractal detected: {fractal_results.get('fractal_detected', False)}")
        print(f"  Approach winner: {approach_comparison['winner']}")
        
        return trial_result
    
    def run_all_trials(self) -> Dict[str, Any]:
        """IMPLEMENTED: Execute all experimental trials across ranges"""
        print("\n" + "="*60)
        print("STARTING EXPERIMENT: Hypothesis Falsification Test")
        print("="*60)
        
        all_trials = []
        trial_count = 0
        
        for prime_range in self.config.prime_ranges:
            print(f"\nPrime Range: [{prime_range[0]:,}, {prime_range[1]:,}]")
            
            for trial_idx in range(self.config.num_trials):
                trial_count += 1
                trial_result = self.run_single_trial(trial_count, prime_range)  # [IMPLEMENTED ✓]
                all_trials.append(trial_result)
        
        # Aggregate results across all trials
        successful_trials = [t for t in all_trials if t['status'] == 'success']
        
        # Calculate summary statistics
        if len(successful_trials) > 0:
            # Distribution comparison stats
            lognorm_wins = sum(1 for t in successful_trials 
                             if t['distribution_comparison']['winner'] == 'lognormal')
            exp_wins = sum(1 for t in successful_trials 
                          if t['distribution_comparison']['winner'] == 'exponential')
            
            # Fractal cascade stats
            fractal_detected_count = sum(1 for t in successful_trials 
                                        if t['fractal_cascade'].get('fractal_detected', False))
            
            # Approach comparison stats
            bottom_up_wins = sum(1 for t in successful_trials 
                                if t['approach_comparison']['winner'] == 'bottom_up')
            top_down_wins = sum(1 for t in successful_trials 
                               if t['approach_comparison']['winner'] == 'top_down')
            
            # Reversed hierarchy stats
            reversed_hierarchy_count = sum(1 for t in successful_trials 
                                          if t['bottom_up_analysis']['reversed_hierarchy'])
            
            summary = {
                'total_trials': len(all_trials),
                'successful_trials': len(successful_trials),
                'failed_trials': len(all_trials) - len(successful_trials),
                'lognormal_wins': lognorm_wins,
                'exponential_wins': exp_wins,
                'lognormal_win_rate': lognorm_wins / len(successful_trials),
                'fractal_detected_count': fractal_detected_count,
                'fractal_detection_rate': fractal_detected_count / len(successful_trials),
                'bottom_up_wins': bottom_up_wins,
                'top_down_wins': top_down_wins,
                'bottom_up_win_rate': bottom_up_wins / len(successful_trials),
                'reversed_hierarchy_count': reversed_hierarchy_count,
                'reversed_hierarchy_rate': reversed_hierarchy_count / len(successful_trials),
            }
        else:
            summary = {
                'total_trials': len(all_trials),
                'successful_trials': 0,
                'error': 'no_successful_trials',
            }
        
        return {
            'experiment_id': self.experiment_id,
            'start_time': str(self.start_time),
            'end_time': str(datetime.now()),
            'config': self.config.to_dict(),  # [IMPLEMENTED ✓]
            'all_trials': all_trials,
            'summary': summary,
        }
    
    def analyze_results(self, all_results: Dict) -> Dict[str, Any]:
        """IMPLEMENTED: Analyze aggregated results to test hypothesis"""
        summary = all_results['summary']
        
        # Test each component of the hypothesis
        
        # 1. Lognormal significantly outperforms exponential (ΔBIC ≥ 10 in ≥70% trials)
        lognorm_win_rate = summary.get('lognormal_win_rate', 0)
        lognormal_supported = lognorm_win_rate >= 0.70
        
        # 2. Fractal cascade structure detected (stable H ∈ [0.7, 0.9])
        fractal_rate = summary.get('fractal_detection_rate', 0)
        fractal_supported = fractal_rate >= 0.50
        
        # 3. Bottom-up shows advantage over top-down
        bottom_up_rate = summary.get('bottom_up_win_rate', 0)
        bottom_up_supported = bottom_up_rate >= 0.50
        
        # 4. Reversed hierarchy pattern replicates
        reversed_rate = summary.get('reversed_hierarchy_rate', 0)
        reversed_supported = reversed_rate >= 0.70
        
        # Overall hypothesis verdict
        components_supported = sum([
            lognormal_supported,
            fractal_supported,
            bottom_up_supported,
            reversed_supported,
        ])
        
        if components_supported >= 3:
            overall_verdict = 'SUPPORTED'
            confidence = 'high' if components_supported == 4 else 'moderate'
        elif components_supported >= 2:
            overall_verdict = 'PARTIALLY_SUPPORTED'
            confidence = 'low'
        else:
            overall_verdict = 'FALSIFIED'
            confidence = 'high'
        
        return {
            'overall_verdict': overall_verdict,
            'confidence': confidence,
            'components': {
                'lognormal_vs_exponential': {
                    'supported': lognormal_supported,
                    'win_rate': lognorm_win_rate,
                    'threshold': 0.70,
                },
                'fractal_cascade': {
                    'supported': fractal_supported,
                    'detection_rate': fractal_rate,
                    'threshold': 0.50,
                },
                'bottom_up_advantage': {
                    'supported': bottom_up_supported,
                    'win_rate': bottom_up_rate,
                    'threshold': 0.50,
                },
                'reversed_hierarchy': {
                    'supported': reversed_supported,
                    'replication_rate': reversed_rate,
                    'threshold': 0.70,
                },
            },
            'components_supported': components_supported,
            'total_components': 4,
        }
    
    def save_results(self, results: Dict, filename: str):
        """IMPLEMENTED: Save experimental results to file"""
        import json
        
        # Add timestamp to results
        results['saved_at'] = str(datetime.now())
        
        # Save full JSON
        json_path = self.config.results_dir / filename
        with open(json_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\n[Results saved to: {json_path}]")
        
        # Save summary as text file
        summary_path = self.config.results_dir / 'summary.txt'
        with open(summary_path, 'w') as f:
            f.write("="*60 + "\n")
            f.write("EXPERIMENT SUMMARY\n")
            f.write("="*60 + "\n\n")
            
            if 'summary' in results:
                summary = results['summary']
                f.write(f"Total Trials: {summary.get('total_trials', 0)}\n")
                f.write(f"Successful: {summary.get('successful_trials', 0)}\n\n")
                
                f.write("Distribution Comparison:\n")
                f.write(f"  Lognormal wins: {summary.get('lognormal_wins', 0)}\n")
                f.write(f"  Exponential wins: {summary.get('exponential_wins', 0)}\n")
                f.write(f"  Lognormal win rate: {summary.get('lognormal_win_rate', 0):.1%}\n\n")
                
                f.write("Fractal Cascade Detection:\n")
                f.write(f"  Detected: {summary.get('fractal_detected_count', 0)}\n")
                f.write(f"  Detection rate: {summary.get('fractal_detection_rate', 0):.1%}\n\n")
                
                f.write("Approach Comparison:\n")
                f.write(f"  Bottom-up wins: {summary.get('bottom_up_wins', 0)}\n")
                f.write(f"  Top-down wins: {summary.get('top_down_wins', 0)}\n")
                f.write(f"  Bottom-up win rate: {summary.get('bottom_up_win_rate', 0):.1%}\n\n")
                
                f.write("Reversed Hierarchy:\n")
                f.write(f"  Detected: {summary.get('reversed_hierarchy_count', 0)}\n")
                f.write(f"  Replication rate: {summary.get('reversed_hierarchy_rate', 0):.1%}\n")
        
        print(f"[Summary saved to: {summary_path}]")


class FindingsGenerator:
    """Generate FINDINGS.md document from experimental results"""
    
    def __init__(self, results: Dict[str, Any]):
        """IMPLEMENTED: Initialize findings generator with experiment results"""
        self.results = results
        
        # Extract key metrics for quick access
        self.summary = results.get('summary', {})
        self.analysis = results.get('analysis', {})
        
        # Extract verdict and components
        self.verdict = self.analysis.get('overall_verdict', 'UNKNOWN')
        self.confidence = self.analysis.get('confidence', 'unknown')
        self.components = self.analysis.get('components', {})
    
    def generate_conclusion(self) -> str:
        """IMPLEMENTED: Generate conclusion section based on results"""
        conclusion = "## CONCLUSION\n\n"
        
        # State verdict clearly
        conclusion += f"**Verdict:** {self.verdict} ({self.confidence} confidence)\n\n"
        
        # Provide clear statement
        if self.verdict == 'SUPPORTED':
            conclusion += "The experimental evidence **SUPPORTS** the hypothesis that the zfifteen/playground repository pioneers an AI-augmented scientific falsification pipeline for prime gap modeling.\n\n"
        elif self.verdict == 'PARTIALLY_SUPPORTED':
            conclusion += "The experimental evidence **PARTIALLY SUPPORTS** the hypothesis with some components validated but others not meeting thresholds.\n\n"
        else:
            conclusion += "The experimental evidence **FALSIFIES** the hypothesis. The claimed patterns do not replicate under rigorous testing.\n\n"
        
        # Key findings summary
        conclusion += "### Key Findings:\n\n"
        
        for component_name, component_data in self.components.items():
            supported = component_data.get('supported', False)
            status = "✓ CONFIRMED" if supported else "✗ NOT CONFIRMED"
            conclusion += f"- **{component_name.replace('_', ' ').title()}**: {status}\n"
        
        conclusion += f"\n**Statistical Summary:** {self.analysis.get('components_supported', 0)}/{self.analysis.get('total_components', 4)} hypothesis components supported\n\n"
        
        return conclusion
    
    def generate_evidence_section(self) -> str:
        """IMPLEMENTED: Generate technical evidence section"""
        evidence = "## TECHNICAL EVIDENCE\n\n"
        
        # 1. Distribution Comparison
        evidence += "### 1. Distribution Comparison: Lognormal vs Exponential\n\n"
        lognorm_comp = self.components.get('lognormal_vs_exponential', {})
        evidence += f"- **Win Rate:** {lognorm_comp.get('win_rate', 0):.1%}\n"
        evidence += f"- **Threshold:** {lognorm_comp.get('threshold', 0):.1%}\n"
        evidence += f"- **Supported:** {lognorm_comp.get('supported', False)}\n\n"
        
        evidence += "| Metric | Value |\n"
        evidence += "|--------|-------|\n"
        evidence += f"| Lognormal wins | {self.summary.get('lognormal_wins', 0)} |\n"
        evidence += f"| Exponential wins | {self.summary.get('exponential_wins', 0)} |\n"
        evidence += f"| Total trials | {self.summary.get('successful_trials', 0)} |\n\n"
        
        # 2. Fractal Cascade Detection
        evidence += "### 2. Fractal Cascade Structure\n\n"
        fractal_comp = self.components.get('fractal_cascade', {})
        evidence += f"- **Detection Rate:** {fractal_comp.get('detection_rate', 0):.1%}\n"
        evidence += f"- **Threshold:** {fractal_comp.get('threshold', 0):.1%}\n"
        evidence += f"- **Supported:** {fractal_comp.get('supported', False)}\n\n"
        
        # 3. Bottom-Up vs Top-Down
        evidence += "### 3. Bottom-Up vs Top-Down Analysis\n\n"
        bottomup_comp = self.components.get('bottom_up_advantage', {})
        evidence += f"- **Bottom-Up Win Rate:** {bottomup_comp.get('win_rate', 0):.1%}\n"
        evidence += f"- **Threshold:** {bottomup_comp.get('threshold', 0):.1%}\n"
        evidence += f"- **Supported:** {bottomup_comp.get('supported', False)}\n\n"
        
        evidence += "| Approach | Wins |\n"
        evidence += "|----------|------|\n"
        evidence += f"| Bottom-Up | {self.summary.get('bottom_up_wins', 0)} |\n"
        evidence += f"| Top-Down | {self.summary.get('top_down_wins', 0)} |\n\n"
        
        # 4. Reversed Hierarchy Replication
        evidence += "### 4. Reversed Hierarchy Pattern Replication\n\n"
        reversed_comp = self.components.get('reversed_hierarchy', {})
        evidence += f"- **Replication Rate:** {reversed_comp.get('replication_rate', 0):.1%}\n"
        evidence += f"- **Threshold:** {reversed_comp.get('threshold', 0):.1%}\n"
        evidence += f"- **Supported:** {reversed_comp.get('supported', False)}\n\n"
        
        return evidence
    
    def generate_methodology_section(self) -> str:
        """IMPLEMENTED: Document experimental methodology"""
        methodology = "## METHODOLOGY\n\n"
        
        config = self.results.get('config', {})
        params = config.get('parameters', {})
        
        methodology += "### Experimental Design\n\n"
        methodology += f"- **Number of Trials:** {params.get('num_trials', 0)} per range\n"
        methodology += f"- **Prime Ranges:** {len(params.get('prime_ranges', []))} ranges tested\n"
        methodology += f"- **Random Seed:** {params.get('seed', 0)} (for reproducibility)\n\n"
        
        methodology += "### Hypothesis Components Tested\n\n"
        methodology += "1. **Lognormal vs Exponential:** Prime gaps better modeled by lognormal than exponential distribution\n"
        methodology += "2. **Fractal Cascade:** Self-similar structure with stable Hurst exponent H ∈ [0.7, 0.9]\n"
        methodology += "3. **Bottom-Up Advantage:** Bottom-up analysis outperforms top-down assumptions\n"
        methodology += "4. **Reversed Hierarchy:** Higher-order moments converge faster (from PR-0005)\n\n"
        
        methodology += "### Statistical Methods\n\n"
        methodology += "- **Distribution Fitting:** Maximum likelihood estimation\n"
        methodology += "- **Goodness-of-Fit:** Kolmogorov-Smirnov test, Anderson-Darling test\n"
        methodology += "- **Model Comparison:** AIC and BIC information criteria\n"
        methodology += "- **Fractal Analysis:** Log-log regression for Hurst exponent\n"
        methodology += "- **Moment Analysis:** Factorial-normalized convergence rates\n\n"
        
        methodology += "### Falsification Criteria\n\n"
        methodology += "Hypothesis **SUPPORTED** if:\n"
        methodology += "- Lognormal wins ≥70% of trials (ΔBIC ≥ 10)\n"
        methodology += "- Fractal cascade detected in ≥50% of trials\n"
        methodology += "- Bottom-up wins ≥50% of comparisons\n"
        methodology += "- Reversed hierarchy replicates in ≥70% of trials\n\n"
        
        methodology += "Hypothesis **FALSIFIED** if ≥2 components fail to meet thresholds.\n\n"
        
        return methodology
    
    def generate_findings_document(self, output_path: Path):
        """IMPLEMENTED: Generate complete FINDINGS.md document"""
        # Generate all sections [ALL IMPLEMENTED ✓]
        conclusion = self.generate_conclusion()
        evidence = self.generate_evidence_section()
        methodology = self.generate_methodology_section()
        
        # Combine in proper order (conclusion first as required)
        document = f"""# FINDINGS: Hypothesis Falsification Test

**Experiment ID:** {self.results.get('experiment_id', 'unknown')}  
**Date:** {self.results.get('end_time', 'unknown')}  
**Repository:** zfifteen/playground  

---

{conclusion}

{evidence}

{methodology}

## RAW DATA

Complete experimental results are available in: `results/experiment_results.json`

## REFERENCES

Related experiments in this repository:
- `PR-0005_reversed_hierarchy_discovery` - Reversed hierarchy pattern
- `falsify/fractal-cascade-structure` - Fractal cascade testing  
- `falsify/lognormal-vs-exponential` - Distribution comparison
- `PR-0003_prime_log_gap_optimized` - Prime generation utilities

---

*This document was automatically generated from experimental data.*
"""
        
        # Write to file
        with open(output_path, 'w') as f:
            f.write(document)
        
        print(f"\n[FINDINGS.md generated: {output_path}]")


def main():
    """IMPLEMENTED: Main entry point for experiment execution"""
    print("\n" + "="*60)
    print("HYPOTHESIS FALSIFICATION TEST")
    print("zfifteen/playground Repository Analysis")
    print("="*60 + "\n")
    
    # Create configuration [IMPLEMENTED ✓]
    print("Initializing experiment configuration...")
    config = ExperimentConfig()
    
    # Save configuration
    config.save(config.data_dir / "config.json")  # [IMPLEMENTED ✓]
    print(f"  Configuration saved to: {config.data_dir / 'config.json'}")
    
    # Initialize pipeline [IMPLEMENTED ✓]
    pipeline = FalsificationPipeline(config)
    
    # Run all trials [IMPLEMENTED ✓]
    print("\nRunning experimental trials...")
    all_results = pipeline.run_all_trials()
    
    # Analyze results [IMPLEMENTED ✓]
    print("\nAnalyzing results...")
    analysis = pipeline.analyze_results(all_results)
    all_results['analysis'] = analysis
    
    # Save results [IMPLEMENTED ✓]
    print("\nSaving results...")
    pipeline.save_results(all_results, 'experiment_results.json')
    
    # Generate FINDINGS.md [IMPLEMENTED ✓]
    print("\nGenerating FINDINGS.md...")
    findings_gen = FindingsGenerator(all_results)
    findings_path = config.base_dir / 'FINDINGS.md'
    findings_gen.generate_findings_document(findings_path)
    
    # Print summary
    print("\n" + "="*60)
    print("EXPERIMENT COMPLETE")
    print("="*60)
    print(f"\nVERDICT: {analysis['overall_verdict']} ({analysis['confidence']} confidence)")
    print(f"Components Supported: {analysis['components_supported']}/{analysis['total_components']}")
    print(f"\nResults saved to: {config.results_dir}")
    print(f"Findings document: {findings_path}")
    print("\n" + "="*60 + "\n")
    
    return 0


if __name__ == "__main__":
    main()
